{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè† House Price Prediction - Exploratory Data Analysis (EDA)\n",
        "\n",
        "## VietAI - Foundations of Machine Learning Final Project\n",
        "\n",
        "**M·ª•c ti√™u:**\n",
        "- Hi·ªÉu r√µ c·∫•u tr√∫c v√† ƒë·∫∑c ƒëi·ªÉm c·ªßa b·ªô d·ªØ li·ªáu House Prices\n",
        "- Ph√¢n t√≠ch th·ªëng k√™ m√¥ t·∫£ c√°c bi·∫øn\n",
        "- Ph√°t hi·ªán missing values v√† outliers\n",
        "- Tr·ª±c quan h√≥a m·ªëi t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn\n",
        "\n",
        "**Dataset:** [Kaggle House Prices Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('../data/raw', exist_ok=True)\n",
        "os.makedirs('../data/processed', exist_ok=True)\n",
        "os.makedirs('../reports', exist_ok=True)\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Custom color palette\n",
        "COLORS = {\n",
        "    'primary': '#2E86AB',\n",
        "    'secondary': '#A23B72',\n",
        "    'accent': '#F18F01',\n",
        "    'success': '#C73E1D',\n",
        "    'dark': '#3B1F2B'\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n",
        "\n",
        "**L∆∞u √Ω:** T·∫£i d·ªØ li·ªáu t·ª´ [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) v√† ƒë·∫∑t trong th∆∞ m·ª•c `../data/raw/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Data files not found!\n",
            "Please download data from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
            "And place train.csv and test.csv in ../data/raw/ folder\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "train_path = '../data/raw/train.csv'\n",
        "test_path = '../data/raw/test.csv'\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    print(f\"‚úÖ Training data loaded: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
        "    print(f\"‚úÖ Test data loaded: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è Data files not found!\")\n",
        "    print(\"Please download data from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\")\n",
        "    print(\"And place train.csv and test.csv in ../data/raw/ folder\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# First look at the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "# First look at the data\n",
        "train_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data info\n",
        "print(\"=\"*60)\n",
        "print(\"DATA INFO\")\n",
        "print(\"=\"*60)\n",
        "train_df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Th·ªëng K√™ M√¥ T·∫£ (Descriptive Statistics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerical columns statistics\n",
        "print(\"üìä TH·ªêNG K√ä C√ÅC BI·∫æN S·ªê\")\n",
        "print(\"=\"*80)\n",
        "train_df.describe().T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical columns\n",
        "categorical_cols = train_df.select_dtypes(include=['object']).columns\n",
        "numerical_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "print(f\"\\nüìå S·ªë l∆∞·ª£ng bi·∫øn s·ªë (Numerical): {len(numerical_cols)}\")\n",
        "print(f\"üìå S·ªë l∆∞·ª£ng bi·∫øn ph√¢n lo·∫°i (Categorical): {len(categorical_cols)}\")\n",
        "print(f\"\\nüî¢ Bi·∫øn s·ªë: {list(numerical_cols[:10])}...\")\n",
        "print(f\"\\nüìù Bi·∫øn ph√¢n lo·∫°i: {list(categorical_cols[:10])}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable (SalePrice) statistics\n",
        "print(\"\\nüéØ BI·∫æN M·ª§C TI√äU: SalePrice\")\n",
        "print(\"=\"*50)\n",
        "print(train_df['SalePrice'].describe())\n",
        "print(f\"\\nüìà Skewness: {train_df['SalePrice'].skew():.4f}\")\n",
        "print(f\"üìä Kurtosis: {train_df['SalePrice'].kurtosis():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ph√¢n T√≠ch Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "def analyze_missing_values(df, name=\"Dataset\"):\n",
        "    \"\"\"Analyze missing values in a DataFrame.\"\"\"\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "    \n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing.index,\n",
        "        'Missing Count': missing.values,\n",
        "        'Missing %': missing_pct.values\n",
        "    })\n",
        "    \n",
        "    missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
        "    missing_df = missing_df.sort_values('Missing %', ascending=False)\n",
        "    \n",
        "    print(f\"\\nüîç {name} - MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"T·ªïng s·ªë c·ªôt c√≥ missing values: {len(missing_df)}\")\n",
        "    print(f\"T·ªïng s·ªë gi√° tr·ªã missing: {missing_df['Missing Count'].sum()}\")\n",
        "    \n",
        "    return missing_df\n",
        "\n",
        "missing_train = analyze_missing_values(train_df, \"Training Data\")\n",
        "missing_train.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize missing values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Bar chart of missing values\n",
        "if len(missing_train) > 0:\n",
        "    top_missing = missing_train.head(15)\n",
        "    ax1 = axes[0]\n",
        "    bars = ax1.barh(top_missing['Column'], top_missing['Missing %'], \n",
        "                    color=COLORS['primary'], edgecolor=COLORS['dark'])\n",
        "    ax1.set_xlabel('Ph·∫ßn trƒÉm Missing (%)', fontsize=12)\n",
        "    ax1.set_title('Top 15 C·ªôt C√≥ Missing Values', fontsize=14, fontweight='bold')\n",
        "    ax1.invert_yaxis()\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, pct in zip(bars, top_missing['Missing %']):\n",
        "        ax1.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
        "                f'{pct:.1f}%', va='center', fontsize=10)\n",
        "\n",
        "# Heatmap of missing values\n",
        "ax2 = axes[1]\n",
        "cols_with_missing = missing_train['Column'].tolist()[:20]\n",
        "if cols_with_missing:\n",
        "    msno_data = train_df[cols_with_missing].isnull()\n",
        "    sns.heatmap(msno_data.T, cbar=True, ax=ax2, cmap='YlOrRd')\n",
        "    ax2.set_title('Missing Values Pattern', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Sample Index')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/missing_values.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ph√¢n T√≠ch Bi·∫øn M·ª•c Ti√™u (Target Variable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Histogram\n",
        "ax1 = axes[0]\n",
        "ax1.hist(train_df['SalePrice'], bins=50, color=COLORS['primary'], \n",
        "         edgecolor=COLORS['dark'], alpha=0.7)\n",
        "ax1.axvline(train_df['SalePrice'].mean(), color=COLORS['secondary'], \n",
        "            linestyle='--', linewidth=2, label=f'Mean: ${train_df[\"SalePrice\"].mean():,.0f}')\n",
        "ax1.axvline(train_df['SalePrice'].median(), color=COLORS['accent'], \n",
        "            linestyle='--', linewidth=2, label=f'Median: ${train_df[\"SalePrice\"].median():,.0f}')\n",
        "ax1.set_xlabel('SalePrice ($)', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_title('Ph√¢n Ph·ªëi Gi√° Nh√†', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# Log-transformed\n",
        "ax2 = axes[1]\n",
        "log_price = np.log1p(train_df['SalePrice'])\n",
        "ax2.hist(log_price, bins=50, color=COLORS['secondary'], \n",
        "         edgecolor=COLORS['dark'], alpha=0.7)\n",
        "ax2.set_xlabel('Log(SalePrice)', fontsize=12)\n",
        "ax2.set_ylabel('Frequency', fontsize=12)\n",
        "ax2.set_title('Ph√¢n Ph·ªëi Log-Transformed', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Q-Q Plot\n",
        "ax3 = axes[2]\n",
        "stats.probplot(log_price, dist=\"norm\", plot=ax3)\n",
        "ax3.set_title('Q-Q Plot (Log SalePrice)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/target_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Skewness g·ªëc: {train_df['SalePrice'].skew():.4f}\")\n",
        "print(f\"üìä Skewness sau log transform: {log_price.skew():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ph√¢n T√≠ch T∆∞∆°ng Quan (Correlation Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation with SalePrice\n",
        "correlations = train_df.select_dtypes(include=[np.number]).corr()['SalePrice'].sort_values(ascending=False)\n",
        "\n",
        "print(\"üîó TOP 15 BI·∫æN T∆Ø∆†NG QUAN CAO NH·∫§T V·ªöI SALEPRICE\")\n",
        "print(\"=\"*50)\n",
        "for col, corr in correlations.head(16).items():\n",
        "    if col != 'SalePrice':\n",
        "        print(f\"{col:25s}: {corr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap for top features\n",
        "top_corr_features = correlations.head(12).index.tolist()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "corr_matrix = train_df[top_corr_features].corr()\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
        "            cmap='RdYlBu_r', center=0, linewidths=0.5,\n",
        "            square=True, ax=ax)\n",
        "\n",
        "ax.set_title('Ma Tr·∫≠n T∆∞∆°ng Quan - Top Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plots for top correlated features\n",
        "top_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', \n",
        "                'FullBath', 'YearBuilt']\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    ax = axes[i]\n",
        "    ax.scatter(train_df[feature], train_df['SalePrice'], \n",
        "               alpha=0.5, c=COLORS['primary'], edgecolors=COLORS['dark'], linewidth=0.5)\n",
        "    \n",
        "    # Add trend line\n",
        "    valid_mask = train_df[feature].notna()\n",
        "    z = np.polyfit(train_df.loc[valid_mask, feature], \n",
        "                   train_df.loc[valid_mask, 'SalePrice'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_line = np.linspace(train_df[feature].min(), train_df[feature].max(), 100)\n",
        "    ax.plot(x_line, p(x_line), color=COLORS['secondary'], linewidth=2, label='Trend')\n",
        "    \n",
        "    corr = train_df[feature].corr(train_df['SalePrice'])\n",
        "    ax.set_xlabel(feature, fontsize=11)\n",
        "    ax.set_ylabel('SalePrice ($)', fontsize=11)\n",
        "    ax.set_title(f'{feature} vs SalePrice\\n(r = {corr:.3f})', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('M·ªëi Quan H·ªá Gi·ªØa Top Features v√† Gi√° Nh√†', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/scatter_plots.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ph√¢n T√≠ch Bi·∫øn Ph√¢n Lo·∫°i (Categorical Features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze categorical features vs SalePrice\n",
        "important_cat_features = ['Neighborhood', 'ExterQual', 'KitchenQual', \n",
        "                          'BldgType', 'HouseStyle', 'SaleCondition']\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(important_cat_features):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Order by median SalePrice\n",
        "    order = train_df.groupby(feature)['SalePrice'].median().sort_values(ascending=False).index\n",
        "    \n",
        "    sns.boxplot(data=train_df, x=feature, y='SalePrice', order=order,\n",
        "                palette='viridis', ax=ax)\n",
        "    \n",
        "    ax.set_xlabel(feature, fontsize=11)\n",
        "    ax.set_ylabel('SalePrice ($)', fontsize=11)\n",
        "    ax.set_title(f'SalePrice by {feature}', fontsize=12, fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.suptitle('Ph√¢n T√≠ch Gi√° Nh√† Theo Bi·∫øn Ph√¢n Lo·∫°i', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/categorical_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ph√°t Hi·ªán Outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect outliers using IQR method\n",
        "def detect_outliers_iqr(df, column):\n",
        "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower) | (df[column] > upper)]\n",
        "    return outliers, lower, upper\n",
        "\n",
        "# Analyze outliers for key numerical features\n",
        "key_features = ['SalePrice', 'GrLivArea', 'LotArea', 'TotalBsmtSF', 'GarageArea']\n",
        "\n",
        "print(\"üîç OUTLIERS ANALYSIS (IQR Method)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for feature in key_features:\n",
        "    if feature in train_df.columns:\n",
        "        outliers, lower, upper = detect_outliers_iqr(train_df, feature)\n",
        "        print(f\"\\n{feature}:\")\n",
        "        print(f\"  - Bounds: [{lower:.2f}, {upper:.2f}]\")\n",
        "        print(f\"  - Number of outliers: {len(outliers)} ({len(outliers)/len(train_df)*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify specific outliers in GrLivArea\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.scatter(train_df['GrLivArea'], train_df['SalePrice'], \n",
        "           c=COLORS['primary'], alpha=0.6, edgecolors=COLORS['dark'])\n",
        "\n",
        "# Highlight potential outliers\n",
        "outlier_mask = (train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 300000)\n",
        "ax.scatter(train_df.loc[outlier_mask, 'GrLivArea'], \n",
        "           train_df.loc[outlier_mask, 'SalePrice'],\n",
        "           c=COLORS['success'], s=100, marker='x', linewidths=3,\n",
        "           label=f'Potential Outliers ({outlier_mask.sum()})')\n",
        "\n",
        "ax.set_xlabel('GrLivArea (sq ft)', fontsize=12)\n",
        "ax.set_ylabel('SalePrice ($)', fontsize=12)\n",
        "ax.set_title('GrLivArea vs SalePrice - Outlier Detection', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/grlivarea_outliers.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Identified {outlier_mask.sum()} potential outliers with large area but low price\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. K·∫øt Lu·∫≠n EDA\n",
        "\n",
        "### Nh·ªØng ph√°t hi·ªán ch√≠nh:\n",
        "1. **Ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ (OverallQual)** l√† y·∫øu t·ªë quan tr·ªçng nh·∫•t ·∫£nh h∆∞·ªüng ƒë·∫øn gi√° nh√†\n",
        "2. **Di·ªán t√≠ch** (GrLivArea, TotalBsmtSF) c√≥ m·ªëi t∆∞∆°ng quan m·∫°nh v·ªõi gi√°\n",
        "3. **Missing values** c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω c·∫©n th·∫≠n, ƒë·∫∑c bi·ªát c√°c c·ªôt li√™n quan ƒë·∫øn ti·ªán √≠ch ƒë·∫∑c bi·ªát (Pool, Fence, Alley)\n",
        "4. **Log transformation** n√™n ƒë∆∞·ª£c √°p d·ª•ng cho bi·∫øn SalePrice ƒë·ªÉ gi·∫£m skewness\n",
        "5. **Feature Engineering** c√≥ th·ªÉ t·∫°o ra c√°c bi·∫øn m·ªõi c√≥ gi√° tr·ªã nh∆∞ TotalSF, HouseAge\n",
        "\n",
        "### B∆∞·ªõc ti·∫øp theo:\n",
        "- Data Preprocessing & Feature Engineering\n",
        "- Model Training & Evaluation\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
